{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "# adding classes folder to system path\n",
    "sys.path.insert(0, os.path.abspath('..') + '/src')\n",
    "from hazm import *\n",
    "import hazm\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# document-based polarity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading documnet-based labeled data\n",
    "doc_data = pd.read_csv('../data/fidibook.csv')\n",
    "# doc_data1 = pd.read_csv('../data/fidibook_hannah.csv')\n",
    "doc_data = pd.concat([doc_data, pd.read_csv('../data/fidibook_hannah.csv')], axis=0)\n",
    "doc_data = pd.concat([doc_data, pd.read_csv('../data/amazon_paperwhite.csv')], axis=0)\n",
    "doc_data = pd.concat([doc_data, pd.read_csv('../data/amazon_10thgeneration.csv')], axis=0)\n",
    "doc_data['polarity'] = doc_data['polarity'].replace(1, '+1').replace(-1, '-1')\n",
    "doc_data = doc_data.reset_index(drop=True)\n",
    "doc_data.drop(doc_data.columns[doc_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "+1    348\n",
      "-1    126\n",
      "Name: polarity, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>auther</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>۶ آبان ۱۴۰۰</td>\n",
       "      <td>کاربر دیجی‌کالا</td>\n",
       "      <td>عالیه</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>۱۹ مهر ۱۴۰۰</td>\n",
       "      <td>کاربر دیجی‌کالا</td>\n",
       "      <td>لطفا هرچه زودتر موجودش کنید</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>۱۷ مهر ۱۴۰۰</td>\n",
       "      <td>متین خدامرادی</td>\n",
       "      <td>تو رو خدا موجودش کنید</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>۱۵ مهر ۱۴۰۰</td>\n",
       "      <td>کاربر دیجی‌کالا</td>\n",
       "      <td>لفطا موجودش کنید</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>۵ مهر ۱۴۰۰</td>\n",
       "      <td>متین ضیایی</td>\n",
       "      <td>بهترین کتاب خونیه که میتونید بخرید چون قیمتش ع...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>قاسم کنگرانی</td>\n",
       "      <td>۲۹ تیر ۱۴۰۰</td>\n",
       "      <td>بهترین وسیله برای مطالعه بخصوص با توجه به صنعت...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>۲۹ تیر ۱۴۰۰</td>\n",
       "      <td>کاربر دیجی‌کالا</td>\n",
       "      <td>خوبه .</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>کاربر دیجی‌کالا</td>\n",
       "      <td>۱۱ تیر ۱۴۰۰</td>\n",
       "      <td>بهترین برای مطالعه کتاب ، تا وایفای رو وصل کرد...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>۱۱ تیر ۱۴۰۰</td>\n",
       "      <td>نازنین دهقان</td>\n",
       "      <td>سال هاست که کتاب خوان استفاده میکنم و این یکی ...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>نازنین دهقان</td>\n",
       "      <td>۲۶ خرداد ۱۴۰۰</td>\n",
       "      <td>با سلام و احترام عکس‌های محصول مربوط به نسخه‌ی...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date           auther  \\\n",
       "0        ۶ آبان ۱۴۰۰  کاربر دیجی‌کالا   \n",
       "1        ۱۹ مهر ۱۴۰۰  کاربر دیجی‌کالا   \n",
       "2        ۱۷ مهر ۱۴۰۰    متین خدامرادی   \n",
       "3        ۱۵ مهر ۱۴۰۰  کاربر دیجی‌کالا   \n",
       "4         ۵ مهر ۱۴۰۰       متین ضیایی   \n",
       "..               ...              ...   \n",
       "469     قاسم کنگرانی      ۲۹ تیر ۱۴۰۰   \n",
       "470      ۲۹ تیر ۱۴۰۰  کاربر دیجی‌کالا   \n",
       "471  کاربر دیجی‌کالا      ۱۱ تیر ۱۴۰۰   \n",
       "472      ۱۱ تیر ۱۴۰۰     نازنین دهقان   \n",
       "473     نازنین دهقان    ۲۶ خرداد ۱۴۰۰   \n",
       "\n",
       "                                                  text polarity  \n",
       "0                                                عالیه       +1  \n",
       "1                          لطفا هرچه زودتر موجودش کنید       +1  \n",
       "2                                تو رو خدا موجودش کنید       +1  \n",
       "3                                     لفطا موجودش کنید       +1  \n",
       "4    بهترین کتاب خونیه که میتونید بخرید چون قیمتش ع...       +1  \n",
       "..                                                 ...      ...  \n",
       "469  بهترین وسیله برای مطالعه بخصوص با توجه به صنعت...       +1  \n",
       "470                                             خوبه .       +1  \n",
       "471  بهترین برای مطالعه کتاب ، تا وایفای رو وصل کرد...       +1  \n",
       "472  سال هاست که کتاب خوان استفاده میکنم و این یکی ...       +1  \n",
       "473  با سلام و احترام عکس‌های محصول مربوط به نسخه‌ی...       +1  \n",
       "\n",
       "[474 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of all comments in dataset\n",
    "print(doc_data.shape[0])\n",
    "#number of positive and negative comments in document-level\n",
    "print(doc_data['polarity'].value_counts())\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity distribution in train set:\n",
      " +1    302\n",
      "-1    101\n",
      "Name: polarity, dtype: int64\n",
      "polarity distribution in test set:\n",
      " +1    46\n",
      "-1    25\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = doc_data.sample(frac=0.85,random_state=20)\n",
    "test_data = doc_data.drop(train_data.index)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "print('polarity distribution in train set:\\n', train_data['polarity'].value_counts())\n",
    "print('polarity distribution in test set:\\n', test_data['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, PUNC_FLAG, STEM_FLAG, STWD_FLAG):\n",
    "    words_dict = {}\n",
    "    stop_words = hazm.utils.stopwords_list(stopwords_file= '/Users/atiyehghm/venv/lib/python3.7/site-packages/hazm/data/stopwords.dat')[:100]\n",
    "    \n",
    "    for m in range(data.shape[0]):\n",
    "        comment = data.at[m, 'text']\n",
    "        comment.replace(\"ي\", \"ی\")\n",
    "        #removing all puctuations\n",
    "        if(PUNC_FLAG):\n",
    "            comment = re.sub(r\"[\\[,.;:\\-@#}?؟{٪٬*!)%’(&$<_>/'»«|+=.\\]]+\\ *\", \" \", comment)\n",
    "        normalizer = Normalizer()\n",
    "        comment = normalizer.normalize(comment)\n",
    "        tokenizer = WordTokenizer()\n",
    "        comment = tokenizer.tokenize(comment)\n",
    "        new_comment = []\n",
    "        #stem all words\n",
    "        if(STEM_FLAG):\n",
    "            stemmer = Stemmer()\n",
    "            for t in comment:\n",
    "                t1 = stemmer.stem(t)\n",
    "                if(STWD_FLAG):\n",
    "                    if t1 not in stop_words:\n",
    "                        if t1 in words_dict.keys():\n",
    "                            words_dict[t1]['frq'] += 1\n",
    "                            word['docs'].add(m)\n",
    "                        else:\n",
    "                            word = {'id':len(words_dict), 'frq':1, 'docs':set()}\n",
    "                            word['docs'].add(m)\n",
    "                            words_dict[t1] = word\n",
    "                        new_comment += [t1]\n",
    "                else:\n",
    "                    if t1 in words_dict.keys():\n",
    "                        words_dict[t1]['frq'] += 1\n",
    "                        word['docs'].add(m)\n",
    "                    else:\n",
    "                        word = {'id':len(words_dict), 'frq':1, 'docs':set()}\n",
    "                        word['docs'].add(m)\n",
    "                        words_dict[t1] = word\n",
    "                        new_comment += [t1]\n",
    "        else:\n",
    "            for t in comment:\n",
    "                if(STWD_FLAG):\n",
    "                    if t not in stop_words:\n",
    "                        if t in words_dict.keys():\n",
    "                            words_dict[t]['frq'] += 1\n",
    "                            word['docs'].add(m)\n",
    "                        else:\n",
    "                            word = {'id':len(words_dict), 'frq':1, 'docs':set()}\n",
    "                            word['docs'].add(m)\n",
    "                            words_dict[t] = word\n",
    "                        new_comment += [t]\n",
    "                else:\n",
    "                    if t in words_dict.keys():\n",
    "                        words_dict[t]['frq'] += 1\n",
    "                        word['docs'].add(m)\n",
    "                    else:\n",
    "                        word = {'id':len(words_dict), 'frq':1, 'docs':set()}\n",
    "                        word['docs'].add(m)\n",
    "                        words_dict[t] = word\n",
    "                        new_comment += [t]\n",
    "        \n",
    "        data.at[m, 'text'] = new_comment\n",
    "    \n",
    "    \n",
    "    return data, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 3534\n"
     ]
    }
   ],
   "source": [
    "data = doc_data.copy()\n",
    "processed_data, vocab = preprocess(data, True, True, False)\n",
    "print('size of vocabulary:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data, PUNC_FLAG, STEM_FLAG, STWD_FLAG):\n",
    "    preprocced_data, words_list = preprocess(data, PUNC_FLAG, STEM_FLAG, STWD_FLAG)\n",
    "    n_docs = preprocced_data.shape[0]\n",
    "    tfidf_vectors = np.zeros((n_docs, len(words_list)))\n",
    "    for m in range(preprocced_data.shape[0]):\n",
    "        comment = preprocced_data.at[m, 'text']\n",
    "        for t in comment:\n",
    "            word = words_list[t]\n",
    "            df = len(word['docs'])\n",
    "            idf = np.log((1+n_docs)/(1+df))\n",
    "            tf = word['frq']\n",
    "            tfidf_vectors[m][word['id']] = tf*(idf+1)\n",
    "        n = norm(tfidf_vectors[m], axis=0, ord=2)\n",
    "        if n != 0:\n",
    "            tfidf_vectors[m] = tfidf_vectors[m]/n\n",
    "    return tfidf_vectors, words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fit(vocab, data, PUNC_FLAG, STEM_FLAG, STWD_FLAG):\n",
    "    preprocced_data, words_list = preprocess(data, PUNC_FLAG, STEM_FLAG, STWD_FLAG)\n",
    "    n_docs = preprocced_data.shape[0]\n",
    "    tfidf_vectors = np.zeros((n_docs, len(vocab)))\n",
    "    for m in range(preprocced_data.shape[0]):\n",
    "        comment = preprocced_data.at[m, 'text']\n",
    "        for t in comment:\n",
    "            if t in vocab.keys():\n",
    "                word = vocab[t]\n",
    "                df = len(word['docs'])\n",
    "                idf = np.log((1+n_docs)/(1+df))\n",
    "                tf = word['frq']\n",
    "                tfidf_vectors[m][word['id']] = tf*(idf+1)\n",
    "        n = norm(tfidf_vectors[m], axis=0, ord=2)\n",
    "        if n != 0:\n",
    "            tfidf_vectors[m] = tfidf_vectors[m]/n\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.copy() \n",
    "train_vectors, vocab = tfidf_vectorizer(train, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3191"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 3191)\n",
      "(71, 3191)\n"
     ]
    }
   ],
   "source": [
    "test = test_data.copy() \n",
    "test_vectors = tfidf_fit(vocab, test,True, True, True)\n",
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm and naive bayes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.394714s; Prediction time: 0.066211s\n",
      "positive:  {'precision': 0.647887323943662, 'recall': 1.0, 'f1-score': 0.7863247863247863, 'support': 46}\n",
      "negative:  {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 25}\n",
      "0.647887323943662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['polarity'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(test_data['polarity'], prediction_linear, output_dict=True)\n",
    "print('positive: ', report['+1'])\n",
    "print('negative: ', report['-1'])\n",
    "accuracy_score = metrics.accuracy_score(prediction_linear, test_data['polarity'])\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.647887323943662, 'recall': 1.0, 'f1-score': 0.7863247863247863, 'support': 46}\n",
      "negative:  {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train = train_vectors\n",
    "y_train = train_data['polarity']\n",
    "X_test = test_vectors\n",
    "y_test = test_data['polarity']\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "report = classification_report(test_data['polarity'], predicted, output_dict=True)\n",
    "print('positive: ', report['+1'])\n",
    "print('negative: ', report['-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence-based polarity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "+1    694\n",
      "-1    345\n",
      "Name: polarity, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>کاملا سبکه  .</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>آگهی و تبلیغات نداره و تا روشن کردم به روز‌رس...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>دوسه‌تا دیکشنری خوب داره برای خواندن داره که ...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>خیلی راضی هستم  .</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>بهترین وسیله برای مطالعه بخصوص با توجه به صنعت...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>خوبه  .</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>بهترین برای مطالعه کتاب ، تا وایفای رو وصل کرد...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>من چون هدفم فقط کتاب خوانی‌بود برام فرقی نداش...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>سال هاست که کتاب خوان استفاده میکنم و این یکی ...</td>\n",
       "      <td>+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>با سلام و احترام عکس‌های محصول مربوط به نسخه‌ی...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text polarity\n",
       "1030                                      کاملا سبکه  .       +1\n",
       "1031   آگهی و تبلیغات نداره و تا روشن کردم به روز‌رس...       +1\n",
       "1032   دوسه‌تا دیکشنری خوب داره برای خواندن داره که ...       +1\n",
       "1033                                  خیلی راضی هستم  .       +1\n",
       "1034  بهترین وسیله برای مطالعه بخصوص با توجه به صنعت...       +1\n",
       "1035                                            خوبه  .       +1\n",
       "1036  بهترین برای مطالعه کتاب ، تا وایفای رو وصل کرد...       +1\n",
       "1037   من چون هدفم فقط کتاب خوانی‌بود برام فرقی نداش...       +1\n",
       "1038  سال هاست که کتاب خوان استفاده میکنم و این یکی ...       +1\n",
       "1039  با سلام و احترام عکس‌های محصول مربوط به نسخه‌ی...       -1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading sentence-based labeled data\n",
    "sent_data = pd.read_csv('../data/sentence_fidibook.csv')\n",
    "sent_data = pd.concat([sent_data, pd.read_csv('../data/fidibook_hannah_sentence.csv')], axis=0)\n",
    "sent_data = pd.concat([sent_data, pd.read_csv('../data/amazon_paperwhite_sentence.csv')], axis=0)\n",
    "sent_data = pd.concat([sent_data, pd.read_csv('../data/amazon_10thgeneration_sentence.csv')], axis=0)\n",
    "sent_data.drop(sent_data[sent_data['polarity'] == 0].index, inplace=True)\n",
    "sent_data.reset_index(drop=True, inplace = True)\n",
    "sent_data['polarity'] = sent_data['polarity'].replace(1, '+1').replace(2, '-1')\n",
    "print(sent_data.shape[0])\n",
    "print(sent_data['polarity'].value_counts())\n",
    "sent_data.drop(sent_data.columns[sent_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "sent_data.at[812, 'polarity'] = '+1'\n",
    "sent_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity distribution in train set:\n",
      " +1    585\n",
      "-1    299\n",
      "Name: polarity, dtype: int64\n",
      "polarity distribution in test set:\n",
      " +1    110\n",
      "-1     46\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_sent_data = sent_data.sample(frac=0.85,random_state=20)\n",
    "test_sent_data = sent_data.drop(train_sent_data.index)\n",
    "train_sent_data.reset_index(drop=True, inplace=True)\n",
    "test_sent_data.reset_index(drop=True, inplace=True)\n",
    "print('polarity distribution in train set:\\n', train_sent_data['polarity'].value_counts())\n",
    "print('polarity distribution in test set:\\n', test_sent_data['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_sent_data.copy() \n",
    "sent_train_vectors, sent_vocab = tfidf_vectorizer(train, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884, 2528)\n",
      "(156, 2528)\n"
     ]
    }
   ],
   "source": [
    "test = test_sent_data.copy() \n",
    "sent_test_vectors = tfidf_fit(sent_vocab, test,True, True, True)\n",
    "print(sent_train_vectors.shape)\n",
    "print(sent_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.037231s; Prediction time: 0.315505s\n",
      "positive:  {'precision': 0.7172413793103448, 'recall': 0.9454545454545454, 'f1-score': 0.8156862745098039, 'support': 110}\n",
      "negative:  {'precision': 0.45454545454545453, 'recall': 0.10869565217391304, 'f1-score': 0.1754385964912281, 'support': 46}\n",
      "0.6987179487179487\n"
     ]
    }
   ],
   "source": [
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(sent_train_vectors, train_sent_data['polarity'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(sent_test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(test_sent_data['polarity'], prediction_linear, output_dict=True)\n",
    "print('positive: ', report['+1'])\n",
    "print('negative: ', report['-1'])\n",
    "accuracy_score = metrics.accuracy_score(prediction_linear, test_sent_data['polarity'])\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.7051282051282052, 'recall': 1.0, 'f1-score': 0.8270676691729323, 'support': 110}\n",
      "negative:  {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train = sent_train_vectors\n",
    "y_train = train_sent_data['polarity']\n",
    "X_test = sent_test_vectors\n",
    "y_test = test_sent_data['polarity']\n",
    "BNB = MultinomialNB()\n",
    "BNB.fit(X_train, y_train)\n",
    "predicted = BNB.predict(X_test)\n",
    "report = classification_report(test_sent_data['polarity'], predicted, output_dict=True)\n",
    "print('positive: ', report['+1'])\n",
    "print('negative: ', report['-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
